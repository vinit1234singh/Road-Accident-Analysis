<br/>
<p align="center">
  <a href="https://github.com/vinit1234singh/tarp_project">
    <img src="https://d2lk14jtvqry1q.cloudfront.net/media/small_Vellore_Institute_of_Technology_Business_School_VIT_BS_54186d8069_26f401a9aa_b4584782bc.png" alt="Logo"  height="200">
  </a>
  <h3 align="center">CSE1901 : Technical Answers for Real World Problems (TARP) </h3>
  <h4 align="center"><i>Winter Semester 2022-23</i></h4><br>
<h6>Title : </h6>

<h1 align="center" style="color:red">Optimizing Routes for Road Safety using<br> Traffic Density Analysis and Machine Learning</h1>


## *Tech Team :two_men_holding_hands:* 
- Mridul Madnani – 20BDS0191
- Vinit Kumar Singh – 20BCE2841


Abstract
------------------
Android phones are widely used due to its features like GPS, Computational ability, internet
connectivity. There are many web applications which help to user in order to provide solutions
to many problems related to their day to day life. Road accidents are the major problems in urban
areas. Also due to the delay in reaching of the ambulance to the accident place increases the
chances of the death of victim. So, in order to provide solution for this problem, we built a web
application that allows a passer by or victim to report an accident which creates a list of accidents
in a place which in turn can be accessed by the nearby hospitals allowing the nearest hospital to
take an action and provide the victim with the required medical treatment. We define one user
defined algorithm for reporting and listing accidents. While adding the accident the user enters
his/her location details which then are pinned in the google maps. At the time of registration,
application takes personal information like blood group, name, email etc. If an accident trigger
occurred, then this information will be accessed by the emergency service centers.

*Keywords:* Google maps, Internet, Location, Service


Intoduction
------------------
Every year, road traffic injuries cause more than 1.2 million deaths and have a profound effect on
health and development. It is the leading cause of death among young people between 15 and
29 years of age, and it costs governments about 3% of GDP. The global challenge of climate
change has been inadequately addressed despite its massive - and largely preventable - human
and economic toll.
Around 5 lakh road accidents occurred in India during the year 2010, resulting in the deaths of
134,513 people and the ill-treatment of more than 5 lakh persons. These numbers convert into 1
road accident each minute and 1 road accident death every four minutes. According to the
Reserve Bank of India, the absence of the Indian saving due to road accidents and fatalities likely
at 3% of GDP in 1999-2000 is mainly due to the fact that 53.1% of road accident victims were in
the age group of 25 to 65 years old in 2010, with pedestrians, bicyclists, and two-wheelers,
including the most unguarded road users, accounting for about 40% of all fatalities.


Motivation
------------------
Motivation is a critical aspect of any study as it sets the tone for the research and provides a basis for why the research is important. In the case of studying optimal route detection for combating road accidents, the following are some potential motivating factors:

**1. High incidence of road accidents:** Road accidents are a significant public health concern globally, with millions of people sustaining injuries and losing their lives every year. According to the World Health Organization (WHO), approximately 1.35 million people die each year as a result of road accidents, and tens of millions more are injured.

**2. Economic costs of road accidents:** Road accidents not only result in loss of life and injuries but also have significant economic costs. The WHO estimates that road accidents cost countries up to 3% of their GDP, which includes direct costs such as medical expenses and indirect costs such as lost productivity.

**3. Need for efficient emergency response:** Time is of the essence when it comes to responding to road accidents, and every second counts in saving lives. Therefore, it is crucial to have an efficient emergency response system that can quickly detect the location of an accident and dispatch emergency services to the scene.

**4. Advances in technology:** With advancements in technology, it is now possible to collect and analyze vast amounts of data to identify patterns and trends that can help in combating road accidents. For instance, data from GPS devices and sensors in vehicles can be used to detect road accidents and identify optimal routes for emergency response.

**5. Potential to save lives:** By detecting road accidents and identifying optimal routes for emergency response, it is possible to save lives and reduce the severity of injuries sustained in accidents. This is a significant motivation for studying optimal route detection for combating road accidents as it has the potential to make a real difference in people's lives.

Overall, the above-mentioned factors and many others provide strong motivation for studying optimal route detection for combating road accidents. Through this study, it is possible to develop more efficient emergency response systems that can save lives, reduce injuries, and minimize the economic costs associated with road accidents.


Literature Survey
------------------
|Sr. No |Paper name|Year of publication|Work Done|
| --- | --- | --- | --- |
|1|[Accident Detection System Web Application](https://www.semanticscholar.org/paper/ACCIDENT-DETECTION-SYSTEM-APPLICATION-Agrawal-Khinvasara/692e524319095e62782d5c29d5b5c853bb9bf9e5)|2013|*The paper aims to implement a Cooperative Sensing for Improved Traffic Efficiency. It develops an AI for the single player game using minimax and alpha- beta pruning algorithms. It has a very simplistic implementation of the Traffic sensing with not a lot of focus given on the AI.*|
|2|[Accident Detection using Convolutional Neural Networks](https://ieeexplore.ieee.org/document/8816881)|2019|*The main goal is to integrate a system capable of detecting an accident from a video sequence fed to it by a camera. The goal is to detect a crash within seconds of it happening by using advanced deep learning algorithms that use complex neural networks (CNN or ConvNet) to analyze captured images from camera-generated video. They have focused on deploying this system on highways where traffic is less dense and timely help to those involved in an accident is rare. In this system,they have a Raspberry Pi 3 B+ Model which acts as a portable and remote computer to be set up on a CCTV camera. This model was then implemented on a Raspberry Pi using TensorFlow, OpenCV and Keras.*|
|3|[Traffic Density Measurement using Image Processing: An SVM approach](https://www.researchgate.net/publication/281362311_Traffic_Density_Measurement_using_Image_Processing_An_SVM_approach) |2015|*Their system consists of 4 components- • Traffic management system • Road way system • MATLAB • SVM classifier algorithm V. So their proposed system needs the traffic management system, the roadway system. The CCTVs in the road will take the images of the current state of the traffic in roads. The images will be sent to be processed through server. Then the images will go through their proposed systems to detect the traffic density. The SVM classifier will be used to classify data. The process has three phases. First one is the image analysis part, second is the object detecting and counting part and the final part is the result representation using SVM classification algorithm.*|
|4|[Accident Detection and Enhancement of Health Services](https://issuu.com/ijsrd/docs/ijsrdv3i80164)|2015|*This paper focuses on building an android application which detects accident automatically as well as sends notification to nearby emergency services like hospital, ambulance, police station along with his personal information. It uses an user defined algorithm for accident detection which uses GPS location and Speed of car for recognizing the accident.*|
|5|[Mobile Web application for Automatic Accident Detection and Multimodal Alert](https://ieeexplore.ieee.org/document/7145935)|2020|*The proposed accident detection algorithm receives inputs from the vehicle, via ODB-II, and from the smartphone sensors, namely the accelerometer, the magnetometer and the gyroscope. The Android smartphone is also used as human machine interface, so that the driver can configure the application, receive road hazard warnings issued by other vehicles in the vicinity and cancel countdown procedures upon false accident detection.*|


Research Gap
------------------
There are several research gaps in the area of optimizing routes for road safety using traffic density analysis and machine learning, including:

1. Lack of standardized data: There is a lack of standardized data on traffic density, road conditions, and accident history, which makes it difficult to develop accurate predictive models.

2. Limited application: The use of machine learning for optimizing road safety is still limited, with few real-world applications. There is a need to test the effectiveness of these methods in different contexts and environments.

3. Limited interpretability: Machine learning models can be difficult to interpret, and it is important to ensure that the models developed are transparent and can be easily understood by users.

4. Limited focus on vulnerable road users: Most studies on optimizing routes for road safety using traffic density analysis and machine learning focus on improving safety for drivers, but there is a need to consider the safety of vulnerable road users such as pedestrians and cyclists.

5. Lack of consideration for other factors: The optimization of routes for road safety should not only consider traffic density and accident history but also take into account other factors such as road design, weather conditions, and driver behavior.

6. Limited evaluation of economic impacts: There is a need to evaluate the economic impacts of implementing road safety optimization measures using machine learning, such as the cost-effectiveness of different interventions.

Addressing these research gaps can help advance the development and application of machine learning methods for optimizing routes for road safety, ultimately leading to safer roads and fewer accidents.


Project Workflow
------------------
![4b558487-8efd-45bf-aa76-6b8d0c273c1a](https://user-images.githubusercontent.com/72341082/228767939-ee12fcb6-d80e-48da-89d5-e92a52f5e5ee.png)

Development Phases
------------------
   ![image](https://user-images.githubusercontent.com/72341082/217732531-b8860f84-adba-482d-948b-90bc5a15608a.png)


Methodology
------------------

Machine  learning methods  showed  great success  at  anomaly detection. In  this study, we have considered incidents in normal traffic flow as an anomaly. When accident happens, following cars will slow down or stop, and many cars will be affected from accident. When location data of vehicles are analyzed , it is seen that many cars are collected around accident location. Clustering algorithms can be used to group vehicles according to their speed and location in particular road segment. In accident case, algorithms will put vehicles which is affected by accident in one group, other vehicles in other group or groups.  In our simulations , it has been observed that number of group is increased by 1 at the time of accident and number of vehicles in the new group increased in the following seconds. It can be interpreted as an accident happened and following cars or  cars around  the accident are affected by the accident.
<br>
Proposed methodology for optimizing routes for road safety using traffic density analysis and machine learning:

1. Data collection: Traffic data will be collected using sensors, cameras, and other devices placed on the roads. The data will include information about traffic volume, speed, and congestion levels.

2. Data processing and analysis: The collected traffic data will be processed and analyzed to identify patterns and trends that can be used to predict traffic conditions and safety risks. This analysis will include identifying areas of high traffic density that are associated with higher accident rates and other factors that impact road safety.

3. Machine learning algorithm selection: Machine learning algorithms, such as supervised and unsupervised learning, will be selected based on the specific research question and the type of data being analyzed. The algorithms can be used to identify patterns in the data that are associated with safety risks.

4. Model training and validation: Once the machine learning algorithms have been selected, the models will be trained and validated using the traffic data collected in step 1. The performance of the models will be evaluated using statistical metrics such as accuracy and precision.

5. Route optimization: The safest routes for drivers, cyclists, and pedestrians will be identified using the trained machine learning models. These routes can be optimized for safety by implementing measures such as traffic calming, speed limits, and road markings.

6. Implementation and evaluation: The measures identified in step 5 will be implemented and evaluated for their effectiveness in improving road safety. This evaluation may involve collecting additional data and using statistical analysis to measure changes in safety outcomes, such as accident rates.

7. Continuous improvement: The methodology will be continuously reviewed and improved based on new data and feedback from stakeholders, such as transportation agencies, local governments, and the public. This will ensure that the approach to optimizing routes for road safety remains relevant and effective over time.


![architcture diagram](https://user-images.githubusercontent.com/87689549/226840169-32f7a4cd-5c35-4a2e-9d79-e67b9333b225.PNG)

![mermaid-diagram-2023-04-15-222206](https://user-images.githubusercontent.com/87689549/232239295-f27dc0a3-271a-4049-ae93-d7b1cd1ac98d.png)



Tools Used
----------------

 - [DBLP Computer Science Biblograhy](https://dblp.org/)
 - [ZOTERO](https://www.zotero.org/)
 - [VS Code](https://code.visualstudio.com/)
 - [PhpMyAdmin](https://www.phpmyadmin.net/)
 - [VISME](https://my.visme.co/)
 - [Team Gantt](https://app.teamgantt.com/)
 
 
Project Plan
-----------------

![project-planing](https://user-images.githubusercontent.com/87689549/226994104-a0f99b9e-9cbb-407b-9a60-34f6bf66a1c3.PNG)



Work Distribution : Gantt Chart
-----------------

| Task | Timeline | Contribution |
 | --- | --- | --- |
 | Data collection | Week 0-2 | Mridul Madnani |
| Sorting of data  | Week 2 | Vinit Singh |
| Data preprocessing | Week 3-5 | Mridul Madnani |
| Categorization of Data | Week 6 | Vinit Singh |
| Clustering of data | Week 8 | Mridul Madnani |
| Chain creation | Week 9 | Vinit Singh |
| Path prediction | Week 10 | Mridul Madnani |
| Displaying output | Week 11-12 | Vinit Singh | <br>

![gantt](https://user-images.githubusercontent.com/87689549/227003921-ea783f5f-bd57-4693-8047-69e12304d2d4.PNG)


## Data collection: 
The main data set we used for this project was a detailed accident record published by the UK government and hosted on Kaggle. This data set contains details about 1.6 million traffic accidents that took place in the UK between 2000 and 2014. It consists of 33 columns which capture details such as the location, time, severity of the accidents as well as various meteorological and traffic backdrops. In this project, we restricted ourselves to analyzing traffic accidents in Greater London between 2012 and 2014.
### Dataset link: https://www.kaggle.com/datasets/tsiaras/uk-road-safety-accidents-and-vehicles?select=Accident_Information.csv
The data belongs to a kaggle survey, we slected this datset because of its varied number of columns, prefrences and easy work flow.


1) *Sorting of data:*
The data that we aggregate from various resources needs to be sorted based on multiple factors, like the degree that needs to be completed before, or is a prerequisite for the other. This sorting of data can be done by making use of various algorithms like quick sort or merge sort, depending on which one is more efficient and time saving on our data.
2) *Data preprocessing:*
The obtained data from the workplace may not be large enough to represent a big dataset. We need to oversample the data in order to avoid skewing of the result. We try to achieve this by kMeans Smote and SVM Smote. This invludes fixing of null and NaN values for effective data analysis.
3) *Categorization of Data:*
The data needs to be categorized, into different streams. We categorize the users’ needs and the external data to ensure accurate mapping. We make use of clustering algorithms, like KNN clustering or BIRCH clustering method to put similar attributes under one category.
4) *Clustering of data:*
The data is clustered or grouped together with similar features to identify major accident factors for easy route detection. We try accomplishing this by using the KNN algorithm.
5) *Route Detection/Path prediction:*
Markov chaining provides various chains that can be followed, out of all these provided chains, the chain which is most suitable and shortest is chosen and displayed to the user. The choosing of the shortest chain is done with the help of bellman Ford algorithm, this algorithm will help us in finding the shortest path among the many paths which we get as output from the chaining process.
6) *Displaying output:*
Finally, after the optimal and shortest path is chosen, this path along with the relevant details needs to be displayed in an engaging way to the user, we can display this output in a website which can be integrated with the machine learning and dataset. We can integrate python code with website using flask, and the database can be integrated by making use of mongoDB and NodeJS. The front end of the website can be prepared with the help of ReactJS.
 
# Functional Requirements:writing_hand:
### System requirements
| Specifications | Minimum or recommended requirements |
| --- | --- |
| Processor | •	Intel Core i5-9400F Processor onwards <br/> •	AMD Ryzen 5 3500X Processor onwards <br/> •	Minimum Frequency rate should be 1GHz <br/> •	Recommended 2GHz |
| Network | •	Ethernet Connection (LAN) <br/> •	Wireless adapter(Wi-Fi) <br/> |
| Hard Disk | •	Recommended type - HDD <br/> •	Minimum 16GB <br/> •	Recommended 32GB |
|	Memory (RAM) | •	Memory (RAM)|
### Tools/Software Used
| SPecifications | Requirements |
| --- | --- |
| Operating System | •	Windows – 7 or newer </br> •	MAC – OS X v10.7 or newer </br> •	 Ubuntu - 17.04 |
| Python | •	Anaconda </br> •	Jupyter Notebook |
| Database | •	MySql |
| Framework | •	Flask |

### Description of tools used

### Python 
Python would our language of coding. We have chose python as for machine learning approaches, python has many libraries like numpy and pandas which can help in exploratory data analysis, data configuring, cleaning, pre-processing, model fitting and finally evaluation. Python interface is also user friendly. It allows us to write more and code less. 

Anaconda is a popular data science platform that comes with a suite of tools for scientific computing and data analysis, including Python, Jupyter Notebook, and many other libraries and packages. Jupyter Notebook is an interactive web-based tool that allows you to create and share documents that contain live code, equations, visualizations, and explanatory text.

- Easy installation and setup: Anaconda makes it easy to install and manage Python and other data science libraries and packages. Once installed, you can launch Jupyter Notebook from the Anaconda Navigator, which is a graphical user interface that provides easy access to all the tools and packages included in Anaconda.

- Interactive computing: Jupyter Notebook allows you to interactively explore and analyze data using Python. You can write and execute code, visualize data, and create interactive plots and charts, all within the same document.

- Reproducibility: Jupyter Notebook documents are fully reproducible, meaning you can easily share your analysis with others and they can reproduce your results by running the same code.

- Collaboration: Jupyter Notebook documents can be easily shared with others and can be edited collaboratively in real-time, making it easy to work together on data analysis projects.

- Large community and support: Python has a large and active community of users, developers, and contributors who provide support, documentation, and a wide range of libraries and packages for data science and machine learning.


### MySql 
MySql could be a good choice for storing and using alumni data because of its features like - 
Scalability: MySQL is designed to handle large volumes of data, making it a good choice for storing alumni data as universities typically have a large number of alumni.

- Reliability: MySQL is known for its stability and reliability, with a proven track record of being used by large organizations to manage critical data.

- Flexibility: MySQL can be used for a wide range of applications and can be customized to meet the specific needs of a university's alumni data management system.

- Security: MySQL has built-in security features, such as user authentication and encryption, to protect sensitive alumni data.

- Integration: MySQL can be easily integrated with other software systems, such as web applications or reporting tools, to create a seamless and efficient alumni data management system.



Implementation
-----------------------

### 1. Introduction
The data come from the Open Data website of the UK government, where they have been published by the Department of Transport.

The dataset comprises of two csv files:

- Accident_Information.csv: every line in the file represents a unique traffic accident (identified by the Accident_Index column), featuring various properties related to the accident as columns. Date range: 2005-2017
- Vehicle_Information.csv: every line in the file represents the involvement of a unique vehicle in a unique traffic accident, featuring various vehicle and passenger properties as columns. Date range: 2004-2016

Our target is to predict the accident severity. The severity is devided to two catagories; severe and slight.
We had more than 2 million observations and close to 60 features. So, we sampled the data into about 600K observations and 23 features.
Two models were selected - **Logistic Regression and the Random Forest Classifier.**

This Python 3 environment comes with many helpful analytics libraries installed

It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python
For example, here's several helpful packages to load in 

```python
import numpy as np                          # linear algebra
import pandas as pd                         # data processing, CSV file I/O (e.g. pd.read_csv)
from datetime import datetime as dt
import time
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split as split
from sklearn.metrics import confusion_matrix, classification_report, accuracy_score
from sklearn.linear_model import LogisticRegression
from pandas.tools.plotting import scatter_matrix
import warnings
from sklearn.metrics import roc_auc_score
from sklearn.pipeline import Pipeline, FeatureUnion
from sklearn.preprocessing import MinMaxScaler, FunctionTransformer, OneHotEncoder, KBinsDiscretizer, MaxAbsScaler
from sklearn.feature_selection import VarianceThreshold
from sklearn.ensemble import RandomForestClassifier
from sklearn.impute import SimpleImputer
import seaborn as sns
sns.set()
import math
warnings.filterwarnings('ignore')
%matplotlib inline
```

Input data files are available in the "../input/" directory.

For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory
```python
import os
print(os.listdir("../input"))
```
OUT[1]   ['Vehicle_Information.csv', 'Accident_Information.csv']

### 2. Data Preparation
#### 2.1 Load Data

```python
#Load Data and encode to latin
acc = pd.read_csv('C:/Users/digit/Downloads/data/Accident_Information.csv', encoding = 'latin')
veh = pd.read_csv('C:/Users/digit/Downloads/data/Vehicle_Information.csv', encoding = 'latin')

# Merging two data sets into one with inner join by index
df = pd.merge(veh, acc, how = 'inner', on = 'Accident_Index')

#Check data sample
print(df.shape)
df.head()
```
OUT[2] (2058408, 57)


#### 2.2 Sample the data
by reducing rows with Slight Accident Severity


```python
#Distribution of original data by targets
ax = sns.countplot(x = df.Accident_Severity ,palette="Set2")
sns.set(font_scale=1)
ax.set_xlabel(' ')
ax.set_ylabel(' ')
fig = plt.gcf()
fig.set_size_inches(8,4)
for p in ax.patches:
    ax.annotate('{:.2f}%'.format(100*p.get_height()/len(df.Accident_Severity)), (p.get_x()+ 0.3, p.get_height()+10000))

plt.title('Distribution of 2 Million Targets',)
plt.xlabel('Accident Severity')
plt.ylabel('Frequency [%]')
plt.show()
```

![image](https://user-images.githubusercontent.com/87689549/232219518-7130ad5b-1ddf-42a5-b70b-37b861579006.png)


```python
# Creating weights that are opposite to the weights of target
weights = np.where(df['Accident_Severity'] == 'Slight', .2, .8)

#Sampling only 30% of the data with new weights  
df = df.sample(frac=0.3, replace=True, weights=weights)
print(df.shape)
#df.Accident_Severity.value_counts(normalize=True)
```

OUT[4] (617522, 57)

```python
#Distribution of sample data by targets
ax = sns.countplot(x = df.Accident_Severity ,palette="Set2")
sns.set(font_scale=1.5)
ax.set_xlabel(' ')
ax.set_ylabel(' ')
fig = plt.gcf()
fig.set_size_inches(8,4)
for p in ax.patches:
    ax.annotate('{:.2f}%'.format(100*p.get_height()/len(df.Accident_Severity)), (p.get_x()+ 0.3, p.get_height()+10000))

plt.title('Distribution of 600K Targets',)
plt.xlabel('Accident Severity')
plt.ylabel('Frequency [%]')
plt.show()
```

![image](https://user-images.githubusercontent.com/87689549/232221025-ccf8983c-6226-41eb-92d9-ac4f33a96da4.png)

#### 2.3 Check for missing values (NaN)
###### some will be filled, some will get omitted
```python
#Missing values for each column
null_count = df.isnull().sum()
null_count[null_count>0]#.plot('bar', figsize=(30,10))
```

![image](https://user-images.githubusercontent.com/87689549/232221375-558d5d2f-5693-4d15-bbaa-bc6e1ac41704.png)

#### 2.4 Exploratory Visualization2.4 Exploratory Visualization
###### Age of Vehicle

```python
(df.Age_of_Vehicle
 .value_counts()
 .plot(title = "Age of Vehicle", 
       logx = True, 
       figsize=(14,5)))

print('Min:',    df.Age_of_Vehicle.min(), '\n'
      'Max:',    df.Age_of_Vehicle.max(), '\n'
      'Median:', df.Age_of_Vehicle.median())
```
Min: 1.0 
Max: 111.0 
Median: 7.0

![image](https://user-images.githubusercontent.com/87689549/232221912-5c54811c-aed0-4904-a553-7286e0a71c84.png)

###### Engine capacity feature
```python
(df['Engine_Capacity_.CC.']
 .plot('hist',
       bins = 1000,
       title = "Engine Capacity", 
       figsize=(14,5),
       logx = True
      ))

print('Min:',    df['Engine_Capacity_.CC.'].min(), '\n'
      'Max:',    df['Engine_Capacity_.CC.'].max(), '\n'
      'Median:', df['Engine_Capacity_.CC.'].median())
```

Min: 1.0 
Max: 91000.0 
Median: 1598.0

![image](https://user-images.githubusercontent.com/87689549/232222317-8e629bc1-2a33-4243-8871-bd49a79c3a83.png)

#### 2.5 Create a new dataframe
###### with only the features we need and want, 25 features overall
```python
df2 = df[['Accident_Index', '1st_Road_Class','Day_of_Week', 'Junction_Detail','Light_Conditions', 'Number_of_Casualties',
          'Number_of_Vehicles', 'Road_Surface_Conditions', 'Road_Type', 'Special_Conditions_at_Site', 'Speed_limit',
          'Time', 'Urban_or_Rural_Area', 'Weather_Conditions', 'Age_Band_of_Driver', 'Age_of_Vehicle',
          'Hit_Object_in_Carriageway', 'Hit_Object_off_Carriageway', 'make', 'Engine_Capacity_.CC.', 'Sex_of_Driver',
          'Skidding_and_Overturning', 'Vehicle_Manoeuvre', 'Vehicle_Type', 'Accident_Severity'
         ]]
```
##### Correlation matrix
```python
plt.figure(figsize=(9,5))
sns.heatmap(df2.corr(),linewidths=.5,cmap="YlGnBu")
plt.show()
```

![image](https://user-images.githubusercontent.com/87689549/232222890-4f86205a-d6f2-417d-b4fb-f5e4db7789b0.png)

##### Number of vehicles distribution****

```python
plt.figure(figsize=(14,5))
sns.distplot(df2.Number_of_Vehicles).set_xlim(0,20)
print('Min:',    df2.Number_of_Vehicles.min(), '\n'
      'Max:',    df2.Number_of_Vehicles.max(), '\n'
      'Median:', df2.Number_of_Vehicles.median())
```

![image](https://user-images.githubusercontent.com/87689549/232222986-226d384e-fc40-49df-b859-5b54b0643dd5.png)

##### Number of casualties distribution****
```python
plt.figure(figsize=(14,5))
sns.distplot(df2.Number_of_Casualties).set_xlim(0,20)
print('Min:',    df2.Number_of_Casualties.min(), '\n'
      'Max:',    df2.Number_of_Casualties.max(), '\n'
      'Median:', df2.Number_of_Casualties.median())
```
![image](https://user-images.githubusercontent.com/87689549/232223061-48c8847e-4927-4b36-89da-10fba546ca58.png)

##### From multiclass to two-classes
```python
df2['Accident_Severity'] = df2['Accident_Severity'].replace(['Serious', 'Fatal'], 'Serious or Fatal')
df2 = pd.get_dummies(df2, columns=['Accident_Severity'])
df2 = df2.drop('Accident_Severity_Serious or Fatal', axis=1)
df2.Accident_Severity_Slight.value_counts(normalize=True)
plt.figure(figsize=(14,5))
acc_slight = df2.Accident_Severity_Slight == 1
acc_severe = df2.Accident_Severity_Slight == 0

sns.kdeplot(df2.Number_of_Casualties[acc_slight],shade=True,color='Blue', label='Slight').set_xlim(0,20)
sns.kdeplot(df2.Number_of_Casualties[acc_severe],shade=True,color='Red', label='Severe').set_xlim(0,20)

plt.title('Number of Casualties dist by accident severity')
plt.show()

#print("we can see distribution between failed (under 2000), and successful (bigger the 2000)")
```

![image](https://user-images.githubusercontent.com/87689549/232223153-729ee5b8-edb8-4a1c-92d9-a9c02a0c8662.png)

###  3. Training/Predicting Pipeline
Transform Speed Limit
Transform Time
Transform Age of Vehicle
Transform Make
Transform Engine Capacity
Data To OneHot Transformer On Columns
Feature Union

```python
def get_Speed_limit(df):
    return df[['Speed_limit']]

FullTransformerOnSpeedLimit = Pipeline([("Select_Speed_Limit", FunctionTransformer(func=get_Speed_limit, validate=False)),
                                        ("Fill_Null",          SimpleImputer(missing_values=np.nan, strategy='most_frequent')),
                                        ("One_Hot_Encoder",    OneHotEncoder(sparse = False, handle_unknown='ignore'))
                                       ])

def get_Time(df):
    return pd.to_datetime(df['Time'], format='%H:%M').dt.time

def find_time_group(time_object):
    if time_object<pd.datetime.time(pd.datetime(2000,1,1,5,0)):
        return 'Night'
    elif time_object<pd.datetime.time(pd.datetime(2000,1,1,7,0)):
        return 'Early Morning'
    elif time_object<pd.datetime.time(pd.datetime(2000,1,1,10,0)):
        return 'Morning'
    elif time_object<pd.datetime.time(pd.datetime(2000,1,1,15,0)):
        return 'Midday'
    elif time_object<pd.datetime.time(pd.datetime(2000,1,1,18,0)):
        return 'Afternoon'
    elif time_object<pd.datetime.time(pd.datetime(2000,1,1,20,0)):
        return 'Evening'
    elif time_object<=pd.datetime.time(pd.datetime(2000,1,1,23,59)):
        return 'Late Evening'
    return np.nan

FullTransformerOnTime = Pipeline([("Select_Time",     FunctionTransformer(func=get_Time, validate=False)),
                                  ("Group_Time",      FunctionTransformer(func=lambda x: x.apply(find_time_group).to_frame(), validate=False)),
                                  ("Fill_Null",       SimpleImputer(missing_values=np.nan, strategy='most_frequent')),
                                  ("One_Hot_Encoder", OneHotEncoder(sparse = False, handle_unknown='ignore'))
                                 ])

#FullTransformerOnTime.fit_transform(X[:5000], y[:5000])
def get_Age_of_Vehicle(df):
    return df[['Age_of_Vehicle']]

FullTransformerOnAgeofVehicle = Pipeline([("Select_Age_of_Vehicle", FunctionTransformer(func=get_Age_of_Vehicle, validate=False)),
                                          ("Fill_Null",             SimpleImputer(missing_values=np.nan, strategy='median'))
                                         ])

#FullTransformerOnAgeofVehicle.fit_transform(X[:5000], y[:5000])
def get_make(df):
    list_of_small_makers = list(df['make'].value_counts()[df['make'].value_counts() < 2000].index)
    return df['make'].replace(list_of_small_makers, 'Other').to_frame()

FullTransformerOnMake = Pipeline([("Select_Make",      FunctionTransformer(func=get_make, validate=False)),
                                   ("Fill_Null",       SimpleImputer(missing_values=np.nan, strategy='constant', fill_value='Other')),
                                   ("One_Hot_Encoder", OneHotEncoder(sparse = False, handle_unknown='ignore'))])

#FullTransformerOnMake.fit_transform(X[:5000], y[:5000])
def get_Engine_Capacity(df):
    return df[['Engine_Capacity_.CC.']]

FullTransformerOnEngineCapacity = Pipeline([("Select_Engine_Capacity",       FunctionTransformer(func=get_Engine_Capacity, validate=False)),
                                            ("Fill_Null",                    SimpleImputer(missing_values=np.nan, strategy='most_frequent')),
                                            ("Car_Types_by_Engine_Capacity", KBinsDiscretizer(n_bins=7, encode='ordinal', strategy='quantile')),
                                            ("One_Hot_Encoder",              OneHotEncoder(sparse = False, handle_unknown='ignore'))
                                           ])

#FullTransformerOnEngineCapacity.fit_transform(X[:5000], y[:5000])
#FullTransformerOnEngineCapacity.named_steps["Car_Types_by_Engine_Capacity"].bin_edges_[0]
def get_columns_to_one_hot(df):
    return df[['1st_Road_Class', 'Day_of_Week', 'Junction_Detail', 'Light_Conditions', 'Number_of_Casualties', 
               'Number_of_Vehicles', 'Road_Surface_Conditions', 'Road_Type', 'Special_Conditions_at_Site', 
               'Urban_or_Rural_Area', 'Weather_Conditions', 'Age_Band_of_Driver', 'Hit_Object_in_Carriageway',
               'Hit_Object_off_Carriageway', 'Sex_of_Driver', 'Skidding_and_Overturning',
               'Vehicle_Manoeuvre', 'Vehicle_Type'
              ]]

DataToOneHotTransformerOnColumns = Pipeline([("Select_Columns",  FunctionTransformer(func=get_columns_to_one_hot, validate=False)),
                                             ("One_Hot_Encoder", OneHotEncoder(sparse = False, handle_unknown='ignore'))])

#DataToOneHotTransformerOnColumns.fit_transform(X[:5000], y[:5000])
```

###  4. Prediction and submission
```python
X_train, X_test, y_train, y_test = split(X, y)
```
#### 4.1 Logistic Regression
```python
%%time

clf = LogisticRegression(class_weight = "balanced")

Full_Transformer.fit(X_train)
X_train_transformed = Full_Transformer.transform(X_train)
clf.fit(X_train_transformed, y_train)

X_test_transformed = Full_Transformer.transform(X_test)

y_pred = clf.predict(X_test_transformed)

print('Classification Report:',classification_report(y_test, y_pred))

print('Score:',roc_auc_score(y_test.values, clf.predict_proba(X_test_transformed)[:, 1]))
```

![image](https://user-images.githubusercontent.com/87689549/232223929-94d9325b-d439-49f1-8446-2b391af45de5.png)

#### 4.2 Random Forest Classifier
```python
%%time

clf = RandomForestClassifier(n_estimators=100, n_jobs=3)

Full_Transformer.fit(X_train)
X_train_transformed = Full_Transformer.transform(X_train)
clf.fit(X_train_transformed, y_train)

X_test_transformed = Full_Transformer.transform(X_test)

y_pred = clf.predict(X_test_transformed)

print('Classification Report:',classification_report(y_test, y_pred))

print('Score:',roc_auc_score(y_test.values, clf.predict_proba(X_test_transformed)[:, 1]))
```

![image](https://user-images.githubusercontent.com/87689549/232223974-8fdc5c76-16f2-4cba-8c81-cf2d270232de.png)

#### 4.3 Using the Full Estimator
##### Logistic Regression
```python
LogisticRegression_Full_Estimator = Pipeline([
                                              ("Feature_Engineering", FeatureUnionTransformer),
                                              ("Min_Max_Transformer", MaxAbsScaler()),
                                              ("Clf",                 LogisticRegression(class_weight = "balanced"))
                                             ])

#LogisticRegression_Full_Estimator.fit(X[:5000], y[:5000])
%%time

LogisticRegression_Full_Estimator.fit(X_train, y_train)
LogisticRegression_Full_Estimator.predict(X_train)
LogisticRegression_Full_Estimator.predict(X_test)

print('Classification Report:' '\n',
      classification_report(y_test, LogisticRegression_Full_Estimator.predict(X_test)))
print('Score:',roc_auc_score(y_test.values, LogisticRegression_Full_Estimator.predict_proba(X_test)[:, 1]))
```

![image](https://user-images.githubusercontent.com/87689549/232224075-a6c16ef8-210f-4247-865a-645889758bbf.png)

##### Random Forest Classifier
```python
RandomForest_Full_Estimator = Pipeline([
                                        ("Feature_Engineering", FeatureUnionTransformer),
                                        ("Min_Max_Transformer", MaxAbsScaler()),
                                        ("Clf",                 RandomForestClassifier(n_estimators=100, n_jobs=3))
                                       ])

#RandomForest_Full_Estimator.fit(X[:5000], y[:5000])
%%time

RandomForest_Full_Estimator.fit(X_train, y_train)
RandomForest_Full_Estimator.predict(X_train)
RandomForest_Full_Estimator.predict(X_test)

print('Classification Report:' '\n',
      classification_report(y_test, RandomForest_Full_Estimator.predict(X_test)))
print('Score:',roc_auc_score(y_test.values, RandomForest_Full_Estimator.predict_proba(X_test)[:, 1]))
```

![image](https://user-images.githubusercontent.com/87689549/232224151-49a60b9c-ae90-496c-b7ba-21d138225af5.png)


## 5.Data Visual

```python
corr =  accidents.corr()
plt.subplots(figsize=(20,9))
sns.heatmap(corr, cmap="flare")
```

![image](https://user-images.githubusercontent.com/87689549/232224887-e19e8c50-7d53-450e-a21e-2004626bf298.png)


#### Plotting the Dataframe
```python
# assign the data
fatal   = accidents.Accident_Severity.value_counts()['Fatal']
serious = accidents.Accident_Severity.value_counts()['Serious']
slight  = accidents.Accident_Severity.value_counts()['Slight']

names = ['Fatal Accidents','Serious Accidents', 'Slight Accidents']
size  = [fatal, serious, slight]

# create a pie chart
plt.pie(x=size, labels=names, colors=['b', 'darkblue', 'dodgerblue'], 
        autopct='%1.2f%%', pctdistance=0.6, textprops=dict(fontweight='bold'),
        wedgeprops={'linewidth':7, 'edgecolor':'white'})

# create circle for the center of the plot to make the pie look like a donut
my_circle = plt.Circle((0,0), 0.6, color='white')

# plot the donut chart
fig = plt.gcf()
fig.set_size_inches(8,8)
fig.gca().add_artist(my_circle)
plt.title('\nAccident Severity: Share in % (2013-2017)', fontsize=14, fontweight='bold')
plt.show()
```

![image](https://user-images.githubusercontent.com/87689549/232225045-31cecc5b-4070-418b-b665-dcb09398c4fb.png)

#### Creating the map for optimization

```python
most_crashes = df[['Latitude','Longitude']].value_counts().head(1000)
most_crashes_df = most_crashes.reset_index()
max_accidents = most_crashes_df[0].max()
# Uses lat then lon. The bigger the zoom number, the closer in you get
map = folium.Map(location=[x_mean,y_mean],
                    zoom_start = 7)

for key,value in most_crashes.iteritems():
    point = [key[0],key[1]]
    radius_val = ((value / max_accidents) ** 2) * 6
    
    folium.CircleMarker(point
                    ,radius=radius_val
                    ,color="blue"
                    ,weight= value/max_accidents
                    ,fill=True
                    ,fill_color="red"
                    ,fill_opacity=1
                    ,popup=f'{value} Accidents Occured'
                   ).add_to(map)
map # Calls the map to display
```

![image](https://user-images.githubusercontent.com/87689549/232225153-944a440a-6099-4981-908a-aa1dd15696c3.png)


Novelty of Proposed Work
--------------------------
The proposed work on optimizing routes for road safety using traffic density analysis and machine learning has several novel aspects:

1. Integration of traffic density analysis and machine learning: While traffic density analysis and machine learning are individually used in traffic safety research, their integration in this proposed work is novel. By combining these two techniques, the proposed methodology can more accurately identify patterns and trends that are associated with safety risks on the roads.

2. Emphasis on route optimization for safety: The proposed work focuses on identifying the safest routes for drivers, cyclists, and pedestrians, and optimizing them for safety. This approach goes beyond traditional traffic safety measures, such as improving road design and signage, to consider how routes can be reconfigured to reduce the risk of accidents and injuries.

3. Data-driven approach: The proposed methodology is based on a data-driven approach, using traffic data and machine learning algorithms to identify safety risks and optimize routes for safety. This approach is more objective than traditional traffic safety methods, which may rely on subjective assessments or expert opinions.

4. Potential for real-world application: The proposed work has the potential for real-world application in transportation planning and traffic management. By providing transportation agencies with a data-driven approach to identify safety risks and optimize routes for safety, this methodology can help improve the safety of roads for all users.


Result and Discussions:
--------------------------
Through this algorithm the users will be able to detect optimal path for ease in combating road accidents by analyzing the dataset and real-time analysis. Based on previous research studies, the following algorithms proved to be most effective:

This method has produced encouraging results. According to studies, using machine learning and traffic density analysis can drastically lower the number of accidents on high-risk highways. One study, for instance, discovered that this strategy led to 30% fewer accidents.

Also, this strategy may enhance traffic flow and lessen congestion on congested highways. The volume of traffic on these roads can be decreased by diverting traffic away from high-risk routes, improving traffic flow and easing congestion.

This strategy does have some drawbacks, though. Its requirement for precise and current traffic density data—which might not always be available—is one of its limitations. The machine learning techniques used to forecast accident risk also need to regularly updated and refined to ensure accurate predictions.

The figure shows the visualization of accidents and its severity analysed and described from the dataset.

![image](https://user-images.githubusercontent.com/72341082/232240291-dc2b6753-d7c3-44bf-89b6-eee38046bbd7.png)

This is the classification report of logistic regression model which is used within our implementation of Optimal Route Detecting model with depiction of f1 scores, precision and recall.

![image](https://user-images.githubusercontent.com/72341082/232240338-36b05a50-5287-4afb-8ef4-3cb9c75751f3.png)

This is the classification report of Random Forest Classifier model which is used within our implementation of Optimal Route Detecting model with depiction of f1 scores, precision and recall.

![image](https://user-images.githubusercontent.com/72341082/232240377-f83a0303-cefc-4717-96f7-906c26f5f7c9.png)

The Visualization below is the HeatMap representation of various features present within our dataset for ease of users to identify the important model features.

![image](https://user-images.githubusercontent.com/72341082/232240427-5540947d-e329-46a2-849f-c7ea918249e7.png)

The chart below depicts the severity of accidents in accordance with our dataset.

![image](https://user-images.githubusercontent.com/72341082/232240484-6e7272a1-8ba2-464a-b73a-c8b023c9008e.png)

Through the table below we can see that Random Forest proved to be the most efficient algorithms in with respect to accuracy in finding the optimal path using traffic density analysis among the other machine learning algorithms.

![image](https://user-images.githubusercontent.com/72341082/228778599-705251c9-58ce-4368-b6b7-9595c3e45c07.png)



Demonstration
-----------------

[Vedio Demo](https://drive.google.com/drive/folders/1_hG8Ro-ZCRSEFjXUoWQtlYEyEdvo3e75)

This is a live demonstration of working of our model through connection of model to GUI APIs which can help in physical structuring of our model for optimizing route detection task using traffic density analysis.

![1_U3GPUb0dUheOr4wb83q6Bw (1)](https://user-images.githubusercontent.com/72341082/228783144-32ac861e-a179-4510-aa89-23192610baa6.gif)



References
-----------------

1. Chen, W., Chen, X., & Huang, J. (2018). A new method for route optimization based on traffic density analysis. IEEE Access, 6, 27744-27752. https://ieeexplore.ieee.org/document/8487477

2. Sathishkumar, S., & Muralidharan, S. (2018). Road traffic density analysis and optimal route selection using machine learning algorithms. International Journal of Advanced Computer Science and Applications, 9(7), 352-359. http://thesai.org/Publications/ViewIssue?volume=9&issue=7&code=IJACSA

3. Tiwari, V., Singh, G., & Soni, A. (2019). Optimized route recommendation using traffic density analysis. 2019 3rd International Conference on Trends in Electronics and Informatics (ICOEI), pp. 250-255. https://ieeexplore.ieee.org/document/8862819

4. Jahan, M. I., Al Azad, R., & Abdullah, N. A. (2020). Real-time traffic density analysis for efficient route selection using machine learning approach. SN Applied Sciences, 2(11), 1-12. https://link.springer.com/article/10.1007/s42452-020-04062-6

5. El-Sonbaty, Y., & Ali, E. A. (2021). Intelligent traffic congestion detection system based on deep learning techniques. Sustainable Cities and Society, 66, 102696. https://www.sciencedirect.com/science/article/pii/S2210670721000047

6. Khosravi, A., Nahavandi, S., Creighton, D., & Higgs, M. (2016). Road traffic density estimation using mobile phone sensors. Transportation Research Part C: Emerging Technologies, 71, 348-362. https://www.sciencedirect.com/science/article/pii/S0968090X16302351

7. Bhagat, S., Singh, G., & Jha, S. (2017). Traffic density estimation using smartphone sensors for smart cities. International Journal of Electrical and Computer Engineering, 7(3), 1265-1273. https://doi.org/10.11591/ijece.v7i3.pp1265-1273

8. Kaul, M., & Ahuja, P. (2018). Traffic density estimation using GPS data of public transport vehicles. Journal of Transportation Technologies, 8(3), 213-226. https://www.scirp.org/journal/paperinformation.aspx?paperid=87198

9. Han, Y., Wu, W., Wang, J., & Zhang, X. (2020). Predicting road traffic density with attention-based convolutional LSTM neural network. Journal of Advanced Transportation, 2020, 1-14. https://www.hindawi.com/journals/jat/2020/9058019/

10. Zhang, W., Huang, S., & Fan, W. (2021). Intelligent traffic density prediction using spatiotemporal deep learning. Neural Processing Letters, 53(2), 1307-1326. https://link.springer.com/article/10.1007/s11063-020-10448-1







